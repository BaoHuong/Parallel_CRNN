{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O7Umq4jD24j6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from numba import cuda, jit\n",
        "import time\n",
        "from PIL import Image\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Simulating loading synthetic word dataset images\n",
        "def load_synthetic_word_dataset(folder_path):\n",
        "    images = []\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith('.png') or filename.endswith('.jpg'):\n",
        "            img_path = os.path.join(folder_path, filename)\n",
        "            img = Image.open(img_path).convert('L')  # convert to grayscale\n",
        "            img = img.resize((128, 32))  # resizing to a fixed size\n",
        "            img_array = np.array(img).flatten() / 255.0  # normalize pixel values\n",
        "            images.append(img_array)\n",
        "    return np.array(images)"
      ],
      "metadata": {
        "id": "YoooTyim26nz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The algorithm in the provided code implements a Dense (or fully connected) layer with a forward_seq method to compute the output based on the input.\n",
        "\n",
        "**Purpose**: To compute the output of a fully connected layer based on the input, weights, and biases.\n",
        "**Method**: Use the dot product to compute the output before applying the ReLU activation function to discard negative values.\n"
      ],
      "metadata": {
        "id": "RKkOTqA03TeR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dense layer implementation\n",
        "class Dense:\n",
        "    def __init__(self, input_size, output_size):\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        self.weight = np.random.rand(input_size, output_size) - 0.5\n",
        "        self.bias = np.random.rand(output_size) - 0.5\n",
        "\n",
        "    def forward_seq(self, input):\n",
        "        output = np.zeros((input.shape[0], self.output_size))\n",
        "        for i in range(input.shape[0]):\n",
        "            for j in range(self.output_size):\n",
        "                output[i, j] = np.dot(input[i], self.weight[:, j]) + self.bias[j]\n",
        "                output[i, j] = max(0, output[i, j])  # ReLU activation\n",
        "        return output"
      ],
      "metadata": {
        "id": "8JZGn75W288z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The provided code implements a function forward_jit using Numba's @jit decorator to optimize the computation.\n",
        "\n",
        "**@jit(nopython=True)**:\n",
        "  * This decorator from Numba is used to compile the function to machine code for better performance.\n",
        "  * The nopython=True option ensures that the function runs in \"nopython\" mode, which means that the code will be compiled to run without relying on the Python interpreter, resulting in significant speedup.\n",
        "  * Due to JIT compilation, forward_jit is expected to perform significantly faster, especially for large input sizes. Numba optimizes the loops and mathematical operations at a lower level.\n"
      ],
      "metadata": {
        "id": "328qdM2u47Y_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@jit(nopython=True)\n",
        "def forward_jit(input, weight, bias):\n",
        "    output = np.zeros((input.shape[0], weight.shape[1]))\n",
        "    for i in range(input.shape[0]):\n",
        "        for j in range(weight.shape[1]):\n",
        "            output[i, j] = np.dot(input[i], weight[:, j]) + bias[j]\n",
        "            output[i, j] = max(0, output[i, j])  # ReLU activation\n",
        "    return output"
      ],
      "metadata": {
        "id": "1Hyxk0z0334e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The function uses a 2D grid and block configuration to determine the threads.\n",
        "This means the work is divided into smaller blocks, each containing multiple threads.\n",
        "\n",
        "* Each thread computes the dot product of one row of the input matrix input with one column of the weight matrix weight. The result of this dot product is then added to the corresponding bias bias[j].\n"
      ],
      "metadata": {
        "id": "l2Y797Hz_s_y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@cuda.jit\n",
        "def forward_cuda(input, weight, bias, output):\n",
        "    i, j = cuda.grid(2)\n",
        "    if i < input.shape[0] and j < weight.shape[1]:\n",
        "        val = 0\n",
        "        for k in range(weight.shape[0]):\n",
        "            val += input[i, k] * weight[k, j]\n",
        "        output[i, j] = max(0, val + bias[j])  # ReLU activation"
      ],
      "metadata": {
        "id": "tc70lIYO-qDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function forward_cuda_3d optimizes the computation of the forward pass of a dense layer using CUDA with 3D grid and block configuration."
      ],
      "metadata": {
        "id": "RPv4bvQT6ZOQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@cuda.jit\n",
        "def forward_cuda_3d(input, weight, bias, output):\n",
        "    il, ir, ic = cuda.grid(3)\n",
        "    if il < input.shape[0] and ir < weight.shape[0] and ic < weight.shape[1]:\n",
        "        val = 0\n",
        "        for k in range(weight.shape[0]):\n",
        "            val += input[il, k] * weight[k, ic]\n",
        "        output[il, ic] = max(0, val + bias[ic])  # ReLU activation"
      ],
      "metadata": {
        "id": "W9poyiB06ZyO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to synthetic word dataset folder\n",
        "folder_path = '/content/Synthetic_Word_Dataset'\n",
        "\n",
        "# Load synthetic word dataset\n",
        "input_data = load_synthetic_word_dataset(folder_path)"
      ],
      "metadata": {
        "id": "A6lOniroALAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Creating Dense layer\n",
        "input_size = input_data.shape[1]\n",
        "output_size = 63  # based on the final dense layer output units\n",
        "dense_layer = Dense(input_size, output_size)"
      ],
      "metadata": {
        "id": "dPSB5JaTAOZj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sequential Execution\n",
        "seq_start = time.time()\n",
        "output_seq = dense_layer.forward_seq(input_data)\n",
        "seq_end = time.time()\n"
      ],
      "metadata": {
        "id": "v4LlMAVsARAY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# JIT Execution\n",
        "jit_start = time.time()\n",
        "output_jit = forward_jit(input_data, dense_layer.weight, dense_layer.bias)\n",
        "jit_end = time.time()"
      ],
      "metadata": {
        "id": "cTDdoBZ0ASQy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CUDA Execution\n",
        "output_cuda = np.zeros((input_data.shape[0], output_size))\n",
        "threadsperblock = (16, 16)\n",
        "blockspergrid_x = int(np.ceil(input_data.shape[0] / threadsperblock[0]))\n",
        "blockspergrid_y = int(np.ceil(output_size / threadsperblock[1]))\n",
        "blockspergrid = (blockspergrid_x, blockspergrid_y)\n",
        "\n",
        "cuda_start = time.time()\n",
        "forward_cuda[blockspergrid, threadsperblock](input_data, dense_layer.weight, dense_layer.bias, output_cuda)\n",
        "cuda_end = time.time()"
      ],
      "metadata": {
        "id": "iQxXMb0jAaUR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CUDA 3D Execution\n",
        "\n",
        "output_cuda_3d = np.zeros((batch_size, output_size))\n",
        "threadsperblock = (8, 8, 8)\n",
        "blockspergrid_x = int(np.ceil(batch_size / threadsperblock[0]))\n",
        "blockspergrid_y = int(np.ceil(input_size / threadsperblock[1]))\n",
        "blockspergrid_z = int(np.ceil(output_size / threadsperblock[2]))\n",
        "blockspergrid = (blockspergrid_x, blockspergrid_y, blockspergrid_z)\n",
        "\n",
        "cuda_start_3d = time.time()\n",
        "forward_cuda_3d[blockspergrid, threadsperblock](input_data, dense_layer.weight, dense_layer.bias, output_cuda)\n",
        "cuda_end_3d = time.time()"
      ],
      "metadata": {
        "id": "-p1SD1LkAf-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Timing and Error Analysis\n",
        "print(\"Dense Layer Execution Times\")\n",
        "print(f\"Time Sequential: {seq_end - seq_start}\")\n",
        "print(f\"Time JIT: {jit_end - jit_start}\")\n",
        "print(f\"Time CUDA: {cuda_end - cuda_start}\")\n",
        "print(f\"Time CUDA 3D: {cuda_end_3d - cuda_start_3d}\")\n",
        "\n",
        "print(f\"Error between Sequential and JIT: {np.sum(np.abs(output_seq - output_jit))}\")\n",
        "print(f\"Error between Sequential and CUDA: {np.sum(np.abs(output_seq - output_cuda))}\")\n",
        "print(f\"Error between Sequential and CUDA 3D: {np.sum(np.abs(output_seq - output_cuda_3d))}\")"
      ],
      "metadata": {
        "id": "QJF8BR5E_7BS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}