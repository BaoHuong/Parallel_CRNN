{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from numba import cuda, jit\n",
    "import time\n",
    "from PIL import Image\n",
    "\n",
    "# Simulating loading synthetic word dataset images\n",
    "def load_synthetic_word_dataset(folder_path):\n",
    "    images = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.png') or filename.endswith('.jpg'):\n",
    "            img_path = os.path.join(folder_path, filename)\n",
    "            img = Image.open(img_path).convert('L')  # convert to grayscale\n",
    "            img = img.resize((128, 32))  # resizing to a fixed size\n",
    "            img_array = np.array(img).flatten() / 255.0  # normalize pixel values\n",
    "            images.append(img_array)\n",
    "    return np.array(images)\n",
    "\n",
    "# Dense layer implementation\n",
    "class Dense:\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.weight = np.random.rand(input_size, output_size) - 0.5\n",
    "        self.bias = np.random.rand(output_size) - 0.5\n",
    "\n",
    "    def forward_seq(self, input):\n",
    "        output = np.zeros((input.shape[0], self.output_size))\n",
    "        for i in range(input.shape[0]):\n",
    "            for j in range(self.output_size):\n",
    "                output[i, j] = np.dot(input[i], self.weight[:, j]) + self.bias[j]\n",
    "                output[i, j] = max(0, output[i, j])  # ReLU activation\n",
    "        return output\n",
    "\n",
    "@jit(nopython=True)\n",
    "def forward_jit(input, weight, bias):\n",
    "    output = np.zeros((input.shape[0], weight.shape[1]))\n",
    "    for i in range(input.shape[0]):\n",
    "        for j in range(weight.shape[1]):\n",
    "            output[i, j] = np.dot(input[i], weight[:, j]) + bias[j]\n",
    "            output[i, j] = max(0, output[i, j])  # ReLU activation\n",
    "    return output\n",
    "\n",
    "@cuda.jit\n",
    "def forward_cuda(input, weight, bias, output):\n",
    "    i, j = cuda.grid(2)\n",
    "    if i < input.shape[0] and j < weight.shape[1]:\n",
    "        val = 0\n",
    "        for k in range(weight.shape[0]):\n",
    "            val += input[i, k] * weight[k, j]\n",
    "        output[i, j] = max(0, val + bias[j])  # ReLU activation\n",
    "\n",
    "# Path to synthetic word dataset folder\n",
    "folder_path = '/content/Synthetic_Word_Dataset'\n",
    "\n",
    "# Load synthetic word dataset\n",
    "input_data = load_synthetic_word_dataset(folder_path)\n",
    "\n",
    "# Creating Dense layer\n",
    "input_size = input_data.shape[1]\n",
    "output_size = 63  # based on the final dense layer output units\n",
    "dense_layer = Dense(input_size, output_size)\n",
    "\n",
    "# Sequential Execution\n",
    "seq_start = time.time()\n",
    "output_seq = dense_layer.forward_seq(input_data)\n",
    "seq_end = time.time()\n",
    "\n",
    "# JIT Execution\n",
    "jit_start = time.time()\n",
    "output_jit = forward_jit(input_data, dense_layer.weight, dense_layer.bias)\n",
    "jit_end = time.time()\n",
    "\n",
    "# CUDA Execution\n",
    "output_cuda = np.zeros((input_data.shape[0], output_size))\n",
    "threadsperblock = (16, 16)\n",
    "blockspergrid_x = int(np.ceil(input_data.shape[0] / threadsperblock[0]))\n",
    "blockspergrid_y = int(np.ceil(output_size / threadsperblock[1]))\n",
    "blockspergrid = (blockspergrid_x, blockspergrid_y)\n",
    "\n",
    "cuda_start = time.time()\n",
    "forward_cuda[blockspergrid, threadsperblock](input_data, dense_layer.weight, dense_layer.bias, output_cuda)\n",
    "cuda_end = time.time()\n",
    "\n",
    "# Timing and Error Analysis\n",
    "print(\"Dense Layer Execution Times\")\n",
    "print(f\"Time Sequential: {seq_end - seq_start}\")\n",
    "print(f\"Time JIT: {jit_end - jit_start}\")\n",
    "print(f\"Time CUDA: {cuda_end - cuda_start}\")\n",
    "\n",
    "print(f\"Error between Sequential and JIT: {np.sum(np.abs(output_seq - output_jit))}\")\n",
    "print(f\"Error between Sequential and CUDA: {np.sum(np.abs(output_seq - output_cuda))}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
