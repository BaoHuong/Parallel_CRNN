{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "O7Umq4jD24j6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from numba import cuda, jit\n",
        "import time\n",
        "from PIL import Image\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "YoooTyim26nz"
      },
      "outputs": [],
      "source": [
        "# Simulating loading synthetic word dataset images\n",
        "def load_synthetic_word_dataset(folder_path):\n",
        "    images = []\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith('.png') or filename.endswith('.jpg'):\n",
        "            img_path = os.path.join(folder_path, filename)\n",
        "            img = Image.open(img_path).convert('L')  # convert to grayscale\n",
        "            img = img.resize((128, 32))  # resizing to a fixed size\n",
        "            img_array = np.array(img).flatten() / 255.0  # normalize pixel values\n",
        "            images.append(img_array)\n",
        "    return np.array(images)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKkOTqA03TeR"
      },
      "source": [
        "The algorithm in the provided code implements a Dense (or fully connected) layer with a forward_seq method to compute the output based on the input.\n",
        "\n",
        "**Purpose**: To compute the output of a fully connected layer based on the input, weights, and biases.\n",
        "**Method**: Use the dot product to compute the output before applying the ReLU activation function to discard negative values.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "8JZGn75W288z"
      },
      "outputs": [],
      "source": [
        "# Dense layer implementation\n",
        "class Dense:\n",
        "    def __init__(self, input_size, output_size):\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        self.weight = np.random.rand(input_size, output_size) - 0.5\n",
        "        self.bias = np.random.rand(output_size) - 0.5\n",
        "\n",
        "    def forward_seq(self, input):\n",
        "        output = np.zeros((input.shape[0], self.output_size))\n",
        "        for i in range(input.shape[0]):\n",
        "            for j in range(self.output_size):\n",
        "                output[i, j] = np.dot(input[i], self.weight[:, j]) + self.bias[j]\n",
        "                output[i, j] = max(0, output[i, j])  # ReLU activation\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "328qdM2u47Y_"
      },
      "source": [
        "The provided code implements a function forward_jit using Numba's @jit decorator to optimize the computation.\n",
        "\n",
        "**@jit(nopython=True)**:\n",
        "  * This decorator from Numba is used to compile the function to machine code for better performance.\n",
        "  * The nopython=True option ensures that the function runs in \"nopython\" mode, which means that the code will be compiled to run without relying on the Python interpreter, resulting in significant speedup.\n",
        "  * Due to JIT compilation, forward_jit is expected to perform significantly faster, especially for large input sizes. Numba optimizes the loops and mathematical operations at a lower level.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "1Hyxk0z0334e"
      },
      "outputs": [],
      "source": [
        "@jit(nopython=True)\n",
        "def forward_jit(input, weight, bias):\n",
        "    output = np.zeros((input.shape[0], weight.shape[1]))\n",
        "    for i in range(input.shape[0]):\n",
        "        for j in range(weight.shape[1]):\n",
        "            output[i, j] = np.dot(input[i], weight[:, j]) + bias[j]\n",
        "            output[i, j] = max(0, output[i, j])  # ReLU activation\n",
        "    return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2Y797Hz_s_y"
      },
      "source": [
        "* The function uses a 2D grid and block configuration to determine the threads.\n",
        "This means the work is divided into smaller blocks, each containing multiple threads.\n",
        "\n",
        "* Each thread computes the dot product of one row of the input matrix input with one column of the weight matrix weight. The result of this dot product is then added to the corresponding bias bias[j].\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "tc70lIYO-qDy"
      },
      "outputs": [],
      "source": [
        "@cuda.jit\n",
        "def forward_cuda(input, weight, bias, output):\n",
        "    i, j = cuda.grid(2)\n",
        "    if i < input.shape[0] and j < weight.shape[1]:\n",
        "        val = 0\n",
        "        for k in range(weight.shape[0]):\n",
        "            val += input[i, k] * weight[k, j]\n",
        "        output[i, j] = max(0, val + bias[j])  # ReLU activation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPv4bvQT6ZOQ"
      },
      "source": [
        "The function forward_cuda_3d optimizes the computation of the forward pass of a dense layer using CUDA with 3D grid and block configuration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "W9poyiB06ZyO"
      },
      "outputs": [],
      "source": [
        "@cuda.jit\n",
        "def forward_cuda_3d(input, weight, bias, output):\n",
        "    x, y, z = cuda.grid(3)\n",
        "    if x < input.shape[0] and y < weight.shape[1] and z < weight.shape[0]:\n",
        "        cuda.atomic.add(output, (x, y), input[x, z] * weight[z, y])\n",
        "        if z == weight.shape[0] - 1:\n",
        "            output[x, y] += bias[y]\n",
        "            output[x, y] = max(0, output[x, y])  # ReLU activation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "A6lOniroALAo"
      },
      "outputs": [],
      "source": [
        "# Path to synthetic word dataset folder\n",
        "folder_path = 'Synthetic_Word_Dataset'\n",
        "\n",
        "# Load synthetic word dataset\n",
        "input_data = load_synthetic_word_dataset(folder_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "dPSB5JaTAOZj"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Creating Dense layer\n",
        "input_size = input_data.shape[1]\n",
        "output_size = 63  # based on the final dense layer output units\n",
        "dense_layer = Dense(input_size, output_size)\n",
        "# Generating synthetic data\n",
        "batch_size = 256\n",
        "input_data = np.random.rand(batch_size, input_size) - 0.5\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "v4LlMAVsARAY"
      },
      "outputs": [],
      "source": [
        "# Sequential Execution\n",
        "seq_start = time.time()\n",
        "output_seq = dense_layer.forward_seq(input_data)\n",
        "seq_end = time.time()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "cTDdoBZ0ASQy"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Wist\\AppData\\Local\\Temp\\ipykernel_632\\402789710.py:6: NumbaPerformanceWarning: \u001b[1m\u001b[1mnp.dot() is faster on contiguous arrays, called on (Array(float64, 1, 'C', False, aligned=True), Array(float64, 1, 'A', False, aligned=True))\u001b[0m\u001b[0m\n",
            "  output[i, j] = np.dot(input[i], weight[:, j]) + bias[j]\n"
          ]
        }
      ],
      "source": [
        "# JIT Execution\n",
        "jit_start = time.time()\n",
        "output_jit = forward_jit(input_data, dense_layer.weight, dense_layer.bias)\n",
        "jit_end = time.time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "iQxXMb0jAaUR"
      },
      "outputs": [
        {
          "ename": "CudaDriverError",
          "evalue": "Driver missing function: cuDeviceGetUuid",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mCudaDriverError\u001b[0m                           Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[15], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m blockspergrid \u001b[38;5;241m=\u001b[39m (blockspergrid_x, blockspergrid_y)\n\u001b[0;32m      8\u001b[0m cuda_start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m----> 9\u001b[0m \u001b[43mforward_cuda\u001b[49m\u001b[43m[\u001b[49m\u001b[43mblockspergrid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreadsperblock\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdense_layer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdense_layer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_cuda\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m cuda_end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
            "File \u001b[1;32mc:\\Users\\Wist\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numba\\cuda\\dispatcher.py:539\u001b[0m, in \u001b[0;36m_LaunchConfiguration.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    538\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs):\n\u001b[1;32m--> 539\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgriddim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblockdim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    540\u001b[0m \u001b[43m                                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msharedmem\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Wist\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numba\\cuda\\dispatcher.py:681\u001b[0m, in \u001b[0;36mCUDADispatcher.call\u001b[1;34m(self, args, griddim, blockdim, stream, sharedmem)\u001b[0m\n\u001b[0;32m    679\u001b[0m     kernel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moverloads\u001b[38;5;241m.\u001b[39mvalues()))\n\u001b[0;32m    680\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 681\u001b[0m     kernel \u001b[38;5;241m=\u001b[39m \u001b[43m_dispatcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDispatcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    683\u001b[0m kernel\u001b[38;5;241m.\u001b[39mlaunch(args, griddim, blockdim, stream, sharedmem)\n",
            "File \u001b[1;32mc:\\Users\\Wist\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numba\\cuda\\dispatcher.py:689\u001b[0m, in \u001b[0;36mCUDADispatcher._compile_for_args\u001b[1;34m(self, *args, **kws)\u001b[0m\n\u001b[0;32m    687\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kws\n\u001b[0;32m    688\u001b[0m argtypes \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtypeof_pyval(a) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m args]\n\u001b[1;32m--> 689\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margtypes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Wist\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numba\\cuda\\dispatcher.py:932\u001b[0m, in \u001b[0;36mCUDADispatcher.compile\u001b[1;34m(self, sig)\u001b[0m\n\u001b[0;32m    929\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_can_compile:\n\u001b[0;32m    930\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompilation disabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 932\u001b[0m kernel \u001b[38;5;241m=\u001b[39m \u001b[43m_Kernel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpy_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtargetoptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    933\u001b[0m \u001b[38;5;66;03m# We call bind to force codegen, so that there is a cubin to cache\u001b[39;00m\n\u001b[0;32m    934\u001b[0m kernel\u001b[38;5;241m.\u001b[39mbind()\n",
            "File \u001b[1;32mc:\\Users\\Wist\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numba\\core\\compiler_lock.py:35\u001b[0m, in \u001b[0;36m_CompilerLock.__call__.<locals>._acquire_compile_lock\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_acquire_compile_lock\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m---> 35\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Wist\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numba\\cuda\\dispatcher.py:82\u001b[0m, in \u001b[0;36m_Kernel.__init__\u001b[1;34m(self, py_func, argtypes, link, debug, lineinfo, inline, fastmath, extensions, max_registers, opt, device)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextensions \u001b[38;5;241m=\u001b[39m extensions \u001b[38;5;129;01mor\u001b[39;00m []\n\u001b[0;32m     77\u001b[0m nvvm_options \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfastmath\u001b[39m\u001b[38;5;124m'\u001b[39m: fastmath,\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mopt\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m3\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m opt \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     80\u001b[0m }\n\u001b[1;32m---> 82\u001b[0m cc \u001b[38;5;241m=\u001b[39m \u001b[43mget_current_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcompute_capability\n\u001b[0;32m     83\u001b[0m cres \u001b[38;5;241m=\u001b[39m compile_cuda(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpy_func, types\u001b[38;5;241m.\u001b[39mvoid, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margtypes,\n\u001b[0;32m     84\u001b[0m                     debug\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdebug,\n\u001b[0;32m     85\u001b[0m                     lineinfo\u001b[38;5;241m=\u001b[39mlineinfo,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     88\u001b[0m                     nvvm_options\u001b[38;5;241m=\u001b[39mnvvm_options,\n\u001b[0;32m     89\u001b[0m                     cc\u001b[38;5;241m=\u001b[39mcc)\n\u001b[0;32m     90\u001b[0m tgt_ctx \u001b[38;5;241m=\u001b[39m cres\u001b[38;5;241m.\u001b[39mtarget_context\n",
            "File \u001b[1;32mc:\\Users\\Wist\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numba\\cuda\\api.py:443\u001b[0m, in \u001b[0;36mget_current_device\u001b[1;34m()\u001b[0m\n\u001b[0;32m    441\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_current_device\u001b[39m():\n\u001b[0;32m    442\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGet current device associated with the current thread\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 443\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcurrent_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdevice\n",
            "File \u001b[1;32mc:\\Users\\Wist\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numba\\cuda\\cudadrv\\devices.py:220\u001b[0m, in \u001b[0;36mget_context\u001b[1;34m(devnum)\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_context\u001b[39m(devnum\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    217\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get the current device or use a device by device number, and\u001b[39;00m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;124;03m    return the CUDA context.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_runtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_or_create_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevnum\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Wist\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numba\\cuda\\cudadrv\\devices.py:138\u001b[0m, in \u001b[0;36m_Runtime.get_or_create_context\u001b[1;34m(self, devnum)\u001b[0m\n\u001b[0;32m    136\u001b[0m attached_ctx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_attached_context()\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attached_ctx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 138\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_or_create_context_uncached\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevnum\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m attached_ctx\n",
            "File \u001b[1;32mc:\\Users\\Wist\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numba\\cuda\\cudadrv\\devices.py:155\u001b[0m, in \u001b[0;36m_Runtime._get_or_create_context_uncached\u001b[1;34m(self, devnum)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m driver\u001b[38;5;241m.\u001b[39mget_active_context() \u001b[38;5;28;01mas\u001b[39;00m ac:\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ac:\n\u001b[1;32m--> 155\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_activate_context_for\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    156\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;66;03m# Get primary context for the active device\u001b[39;00m\n\u001b[0;32m    158\u001b[0m         ctx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgpus[ac\u001b[38;5;241m.\u001b[39mdevnum]\u001b[38;5;241m.\u001b[39mget_primary_context()\n",
            "File \u001b[1;32mc:\\Users\\Wist\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numba\\cuda\\cudadrv\\devices.py:176\u001b[0m, in \u001b[0;36m_Runtime._activate_context_for\u001b[1;34m(self, devnum)\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_activate_context_for\u001b[39m(\u001b[38;5;28mself\u001b[39m, devnum):\n\u001b[0;32m    175\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m--> 176\u001b[0m         gpu \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgpus\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdevnum\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    177\u001b[0m         newctx \u001b[38;5;241m=\u001b[39m gpu\u001b[38;5;241m.\u001b[39mget_primary_context()\n\u001b[0;32m    178\u001b[0m         \u001b[38;5;66;03m# Detect unexpected context switch\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\Wist\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numba\\cuda\\cudadrv\\devices.py:40\u001b[0m, in \u001b[0;36m_DeviceList.__getitem__\u001b[1;34m(self, devnum)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, devnum):\n\u001b[0;32m     37\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;124;03m    Returns the context manager for device *devnum*.\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlst\u001b[49m[devnum]\n",
            "File \u001b[1;32mc:\\Users\\Wist\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numba\\cuda\\cudadrv\\devices.py:27\u001b[0m, in \u001b[0;36m_DeviceList.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlst\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;66;03m# Device list is not initialized.\u001b[39;00m\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;66;03m# Query all CUDA devices.\u001b[39;00m\n\u001b[0;32m     26\u001b[0m     numdev \u001b[38;5;241m=\u001b[39m driver\u001b[38;5;241m.\u001b[39mget_device_count()\n\u001b[1;32m---> 27\u001b[0m     gpus \u001b[38;5;241m=\u001b[39m [_DeviceContextManager(\u001b[43mdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevid\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     28\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m devid \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(numdev)]\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;66;03m# Define \"lst\" to avoid re-initialization\u001b[39;00m\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlst \u001b[38;5;241m=\u001b[39m gpus\n",
            "File \u001b[1;32mc:\\Users\\Wist\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numba\\cuda\\cudadrv\\driver.py:415\u001b[0m, in \u001b[0;36mDriver.get_device\u001b[1;34m(self, devnum)\u001b[0m\n\u001b[0;32m    413\u001b[0m dev \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevices\u001b[38;5;241m.\u001b[39mget(devnum)\n\u001b[0;32m    414\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dev \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 415\u001b[0m     dev \u001b[38;5;241m=\u001b[39m \u001b[43mDevice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevnum\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    416\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevices[devnum] \u001b[38;5;241m=\u001b[39m dev\n\u001b[0;32m    417\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m weakref\u001b[38;5;241m.\u001b[39mproxy(dev)\n",
            "File \u001b[1;32mc:\\Users\\Wist\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numba\\cuda\\cudadrv\\driver.py:604\u001b[0m, in \u001b[0;36mDevice.__init__\u001b[1;34m(self, devnum)\u001b[0m\n\u001b[0;32m    602\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    603\u001b[0m     uuid \u001b[38;5;241m=\u001b[39m cu_uuid()\n\u001b[1;32m--> 604\u001b[0m     \u001b[43mdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuDeviceGetUuid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43muuid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    605\u001b[0m     uuid_vals \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mbytes\u001b[39m(uuid))\n\u001b[0;32m    607\u001b[0m b \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%02x\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n",
            "File \u001b[1;32mc:\\Users\\Wist\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numba\\cuda\\cudadrv\\driver.py:326\u001b[0m, in \u001b[0;36mDriver._ctypes_wrap_fn.<locals>.safe_cuda_api_call\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m    324\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msafe_cuda_api_call\u001b[39m(\u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m    325\u001b[0m     _logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcall driver api: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m, libfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m--> 326\u001b[0m     retcode \u001b[38;5;241m=\u001b[39m \u001b[43mlibfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_ctypes_error(fname, retcode)\n",
            "File \u001b[1;32mc:\\Users\\Wist\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numba\\cuda\\cudadrv\\driver.py:377\u001b[0m, in \u001b[0;36mDriver._find_api.<locals>.absent_function\u001b[1;34m(*args, **kws)\u001b[0m\n\u001b[0;32m    376\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mabsent_function\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkws):\n\u001b[1;32m--> 377\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CudaDriverError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDriver missing function: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
            "\u001b[1;31mCudaDriverError\u001b[0m: Driver missing function: cuDeviceGetUuid"
          ]
        }
      ],
      "source": [
        "# CUDA Execution\n",
        "output_cuda = np.zeros((input_data.shape[0], output_size))\n",
        "threadsperblock = (16, 16)\n",
        "blockspergrid_x = int(np.ceil(input_data.shape[0] / threadsperblock[0]))\n",
        "blockspergrid_y = int(np.ceil(output_size / threadsperblock[1]))\n",
        "blockspergrid = (blockspergrid_x, blockspergrid_y)\n",
        "\n",
        "cuda_start = time.time()\n",
        "forward_cuda[blockspergrid, threadsperblock](input_data, dense_layer.weight, dense_layer.bias, output_cuda)\n",
        "cuda_end = time.time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-p1SD1LkAf-v"
      },
      "outputs": [],
      "source": [
        "# CUDA 3D Execution\n",
        "\n",
        "output_cuda_3d = np.zeros((batch_size, output_size))\n",
        "threadsperblock = (8, 8, 8)\n",
        "blockspergrid_x = int(np.ceil(batch_size / threadsperblock[0]))\n",
        "blockspergrid_y = int(np.ceil(input_size / threadsperblock[1]))\n",
        "blockspergrid_z = int(np.ceil(output_size / threadsperblock[2]))\n",
        "blockspergrid = (blockspergrid_x, blockspergrid_y, blockspergrid_z)\n",
        "\n",
        "cuda_start_3d = time.time()\n",
        "forward_cuda_3d[blockspergrid, threadsperblock](input_data, dense_layer.weight, dense_layer.bias, output_cuda)\n",
        "cuda_end_3d = time.time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QJF8BR5E_7BS"
      },
      "outputs": [],
      "source": [
        "# Timing and Error Analysis\n",
        "print(\"Dense Layer Execution Times\")\n",
        "print(f\"Time Sequential: {seq_end - seq_start}\")\n",
        "print(f\"Time JIT: {jit_end - jit_start}\")\n",
        "print(f\"Time CUDA: {cuda_end - cuda_start}\")\n",
        "print(f\"Time CUDA 3D: {cuda_end_3d - cuda_start_3d}\")\n",
        "\n",
        "print(f\"Error between Sequential and JIT: {np.sum(np.abs(output_seq - output_jit))}\")\n",
        "print(f\"Error between Sequential and CUDA: {np.sum(np.abs(output_seq - output_cuda))}\")\n",
        "print(f\"Error between Sequential and CUDA 3D: {np.sum(np.abs(output_seq - output_cuda_3d))}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
